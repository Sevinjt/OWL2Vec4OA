{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355a241a",
   "metadata": {},
   "source": [
    "# Owl2vec_star_ext\n",
    "In this process, we get ontology files and to change it to embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d810e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sev_s\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "INFO: 1\n",
      "INFO: \n",
      " Access the ontology ...\n",
      "INFO: There are 413 triples in the ontology\n",
      "* Owlready2 * Creating new ontology Conference <./case_studies/Data/Conference.owl#>.\n",
      "* Owlready2 * ADD TRIPLE ./case_studies/Data/Conference.owl http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2002/07/owl#Ontology\n",
      "* Owlready2 *     ...loading ontology Conference from ./case_studies/Data/Conference.owl...\n",
      "* OwlReady2 * Importing 356 object triples from ontology ./case_studies/Data/Conference.owl# ...\n",
      "* OwlReady2 * Importing 4 data triples from ontology ./case_studies/Data/Conference.owl# ...\n",
      "* Owlready2 *     ...0 properties found: \n",
      "INFO: There are 774 triples in the ontology\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 0.08215475082397461 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.06694722175598145 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 0.11723637580871582 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.005518913269042969 seconds \n",
      "INFO: \tExtracting triples associated to assignedByReviewer\n",
      "INFO: \t\tTime extracting triples for property: 0.09050846099853516 seconds \n",
      "INFO: \tExtracting triples associated to hasDecision\n",
      "INFO: \t\tTime extracting triples for property: 0.09141755104064941 seconds \n",
      "INFO: \tExtracting triples associated to readByReviewer\n",
      "INFO: \t\tTime extracting triples for property: 0.0881345272064209 seconds \n",
      "INFO: \tExtracting triples associated to readByMeta-Reviewer\n",
      "INFO: \t\tTime extracting triples for property: 0.08562421798706055 seconds \n",
      "INFO: \tExtracting triples associated to hasConflictOfInterest\n",
      "INFO: \t\tTime extracting triples for property: 0.09186434745788574 seconds \n",
      "INFO: \tExtracting triples associated to finalizePaperAssignment\n",
      "INFO: \t\tTime extracting triples for property: 0.0958566665649414 seconds \n",
      "INFO: \tExtracting triples associated to paperAssignmentFinalizedBy\n",
      "INFO: \t\tTime extracting triples for property: 0.09074831008911133 seconds \n",
      "INFO: \tExtracting triples associated to hasConferenceMember\n",
      "INFO: \t\tTime extracting triples for property: 0.09829306602478027 seconds \n",
      "INFO: \tExtracting triples associated to memberOfConference\n",
      "INFO: \t\tTime extracting triples for property: 0.10808658599853516 seconds \n",
      "INFO: \tExtracting triples associated to runPaperAssignmentTools\n",
      "INFO: \t\tTime extracting triples for property: 0.09790658950805664 seconds \n",
      "INFO: \tExtracting triples associated to paperAssignmentToolsRunBy\n",
      "INFO: \t\tTime extracting triples for property: 0.0887303352355957 seconds \n",
      "INFO: \tExtracting triples associated to enableVirtualMeeting\n",
      "INFO: \t\tTime extracting triples for property: 0.08929944038391113 seconds \n",
      "INFO: \tExtracting triples associated to virtualMeetingEnabledBy\n",
      "INFO: \t\tTime extracting triples for property: 0.11263751983642578 seconds \n",
      "INFO: \tExtracting triples associated to hasProgramCommitteeMember\n",
      "INFO: \t\tTime extracting triples for property: 0.14164400100708008 seconds \n",
      "INFO: \tExtracting triples associated to memberOfProgramCommittee\n",
      "INFO: \t\tTime extracting triples for property: 0.09516644477844238 seconds \n",
      "INFO: \tExtracting triples associated to startReviewerBidding\n",
      "INFO: \t\tTime extracting triples for property: 0.10710811614990234 seconds \n",
      "INFO: \tExtracting triples associated to reviewerBiddingStartedBy\n",
      "INFO: \t\tTime extracting triples for property: 0.09583640098571777 seconds \n",
      "INFO: \tExtracting triples associated to assignReviewer\n",
      "INFO: \t\tTime extracting triples for property: 0.09060382843017578 seconds \n",
      "INFO: \tExtracting triples associated to assignedByAdministrator\n",
      "INFO: \t\tTime extracting triples for property: 0.09151959419250488 seconds \n",
      "INFO: \tExtracting triples associated to reviewCriteriaEnteredBy\n",
      "INFO: \t\tTime extracting triples for property: 0.10773134231567383 seconds \n",
      "INFO: \tExtracting triples associated to enterReviewCriteria\n",
      "INFO: \t\tTime extracting triples for property: 0.1345815658569336 seconds \n",
      "INFO: \tExtracting triples associated to hasCo-author\n",
      "INFO: \t\tTime extracting triples for property: 0.1416780948638916 seconds \n",
      "INFO: \tExtracting triples associated to co-writePaper\n",
      "INFO: \t\tTime extracting triples for property: 0.12977957725524902 seconds \n",
      "INFO: \tExtracting triples associated to rejectPaper\n",
      "INFO: \t\tTime extracting triples for property: 0.23105597496032715 seconds \n",
      "INFO: \tExtracting triples associated to rejectedBy\n",
      "INFO: \t\tTime extracting triples for property: 0.09461855888366699 seconds \n",
      "INFO: \tExtracting triples associated to endReview\n",
      "INFO: \t\tTime extracting triples for property: 0.10231232643127441 seconds \n",
      "INFO: \tExtracting triples associated to hasBid\n",
      "INFO: \t\tTime extracting triples for property: 0.1032710075378418 seconds \n",
      "INFO: \tExtracting triples associated to adjustBid\n",
      "INFO: \t\tTime extracting triples for property: 0.12033748626708984 seconds \n",
      "INFO: \tExtracting triples associated to adjustedBy\n",
      "INFO: \t\tTime extracting triples for property: 0.12188124656677246 seconds \n",
      "INFO: \tExtracting triples associated to readPaper\n",
      "INFO: \t\tTime extracting triples for property: 0.09777188301086426 seconds \n",
      "INFO: \tExtracting triples associated to hardcopyMailingManifestsPrintedBy\n",
      "INFO: \t\tTime extracting triples for property: 0.10304641723632812 seconds \n",
      "INFO: \tExtracting triples associated to printHardcopyMailingManifests\n",
      "INFO: \t\tTime extracting triples for property: 0.1208353042602539 seconds \n",
      "INFO: \tExtracting triples associated to markConflictOfInterest\n",
      "INFO: \t\tTime extracting triples for property: 0.1566464900970459 seconds \n",
      "INFO: \tExtracting triples associated to enterConferenceDetails\n",
      "INFO: \t\tTime extracting triples for property: 0.13714146614074707 seconds \n",
      "INFO: \tExtracting triples associated to detailsEnteredBy\n",
      "INFO: \t\tTime extracting triples for property: 0.13523221015930176 seconds \n",
      "INFO: \tExtracting triples associated to hasBeenAssigned\n",
      "INFO: \t\tTime extracting triples for property: 0.1070091724395752 seconds \n",
      "INFO: \tExtracting triples associated to assignedTo\n",
      "INFO: \t\tTime extracting triples for property: 0.1221170425415039 seconds \n",
      "INFO: \tExtracting triples associated to assignExternalReviewer\n",
      "INFO: \t\tTime extracting triples for property: 0.16230559349060059 seconds \n",
      "INFO: \tExtracting triples associated to setMaxPapers\n",
      "INFO: \t\tTime extracting triples for property: 0.12010478973388672 seconds \n",
      "INFO: \tExtracting triples associated to hasSubjectArea\n",
      "INFO: \t\tTime extracting triples for property: 0.13769102096557617 seconds \n",
      "INFO: \tExtracting triples associated to submitPaper\n",
      "INFO: \t\tTime extracting triples for property: 0.1253364086151123 seconds \n",
      "INFO: \tExtracting triples associated to acceptPaper\n",
      "INFO: \t\tTime extracting triples for property: 0.1214437484741211 seconds \n",
      "INFO: \tExtracting triples associated to acceptedBy\n",
      "INFO: \t\tTime extracting triples for property: 0.10003924369812012 seconds \n",
      "INFO: \tExtracting triples associated to hasAuthor\n",
      "INFO: \t\tTime extracting triples for property: 0.11514163017272949 seconds \n",
      "INFO: \tExtracting triples associated to writePaper\n",
      "INFO: \t\tTime extracting triples for property: 0.09882330894470215 seconds \n",
      "INFO: \tExtracting triples associated to addedBy\n",
      "INFO: \t\tTime extracting triples for property: 0.12013435363769531 seconds \n",
      "INFO: \tExtracting triples associated to addProgramCommitteeMember\n",
      "INFO: \t\tTime extracting triples for property: 0.2063138484954834 seconds \n",
      "INFO: \tExtracting triples associated to writtenBy\n",
      "INFO: \t\tTime extracting triples for property: 0.10930681228637695 seconds \n",
      "INFO: \tExtracting triples associated to writeReview\n",
      "INFO: \t\tTime extracting triples for property: 0.11713957786560059 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.2860124111175537 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 304\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 305\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 314\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 307\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 306\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 308\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 310\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 311\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 312\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 316\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 327\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 318\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 319\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 322\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 325\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 326\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 332\n",
      "\n",
      "* Owlready2 * Warning: ignoring cyclic type of, involving storid 338\n",
      "\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 0.004508018493652344 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 0.13319873809814453 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 0.06262373924255371 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.017676591873168945 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 0.15814685821533203 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.010512113571166992 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.0 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 0.003502368927001953 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 0.2725670337677002 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: \n",
      "\n",
      "\n",
      "Extraction time for ontologies:6.95\n",
      "* Owlready2 * Creating new ontology projection.ttl <./caches_cmt/projection.ttl#>.\n",
      "* Owlready2 * ADD TRIPLE ./caches_cmt/projection.ttl http://www.w3.org/1999/02/22-rdf-syntax-ns#type http://www.w3.org/2002/07/owl#Ontology\n",
      "* Owlready2 *     ...loading ontology projection.ttl from ./caches_cmt/projection.ttl...\n",
      "* OwlReady2 * Importing 356 object triples from ontology ./caches_cmt/projection.ttl# ...\n",
      "* OwlReady2 * Importing 4 data triples from ontology ./caches_cmt/projection.ttl# ...\n",
      "* Owlready2 *     ...0 properties found: \n",
      "INFO: There are 1135 triples in the ontology\n",
      "INFO: \n",
      "Calculate the ontology projection ...\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 0.13790321350097656 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.026579618453979492 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 0.22037386894226074 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.018529891967773438 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.0009975433349609375 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 0.0010023117065429688 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 0.20958828926086426 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: Projection saved into turtle file: ./caches_cmt/projection.ttl\n",
      "INFO: \n",
      "Extract classes and individuals ...\n",
      "INFO: Creating ontology graph projection...\n",
      "INFO: \tExtracting subsumption triples\n",
      "INFO: \t\tTime extracting subsumption: 0.09489870071411133 seconds \n",
      "INFO: \tExtracting equivalence triples\n",
      "INFO: \t\tTime extracting equivalences: 0.018667936325073242 seconds \n",
      "INFO: \tExtracting class membership triples.\n",
      "INFO: \t\tTime extracting class membership: 0.2100062370300293 seconds \n",
      "INFO: \tExtracting sameAs triples\n",
      "INFO: \t\tTime extracting sameAs: 0.01102590560913086 seconds \n",
      "INFO: \tExtracting data property assertions\n",
      "INFO: \t\tTime extracting data property assertions: 0.0010101795196533203 seconds \n",
      "INFO: \tExtracting complex equivalence axioms\n",
      "INFO: \t\tTime extracting complex equivalence axioms: 0.001004934310913086 seconds \n",
      "INFO: \tExtracting annotations.\n",
      "INFO: \t\tTime extracting annotations: 0.21171283721923828 seconds \n",
      "INFO: Projection created into a Graph object (RDFlib library)\n",
      "INFO: \n",
      "Extract axioms ...\n",
      "INFO: \n",
      "Extract annotations ...\n",
      "INFO: \n",
      "Generate URI document ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './case_studies/Data/ALIN-cmt-conference.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mowl2vec4mappings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Parameters:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ontology_file1\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# ontology_file2\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# mix_doc\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# but there are some parameters you can change in config file\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m gensim_model \u001b[38;5;241m=\u001b[39m \u001b[43mowl2vec4mappings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_owl2vec_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_file1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./case_studies/Data/cmt.owl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43montology_file2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./case_studies/Data/Conference.owl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./case_studies/Data/ALIN-cmt-conference.tsv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./default1.cfg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                                \u001b[49m\u001b[43muri_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmix_doc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec4mappings.py:45\u001b[0m, in \u001b[0;36mextract_owl2vec_model\u001b[1;34m(ontology_file1, ontology_file2, mapping_file, config_file, uri_doc, lit_doc, mix_doc)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     43\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 45\u001b[0m model_ \u001b[38;5;241m=\u001b[39m \u001b[43m__perform_ontology_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec4mappings.py:195\u001b[0m, in \u001b[0;36m__perform_ontology_embedding\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURI_Doc\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURI_Doc\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    193\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerate URI document ...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m     walks_,instances \u001b[38;5;241m=\u001b[39m \u001b[43mget_rdf2vec_walks\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43montology_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwalker_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDOCUMENT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwalker\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mwalk_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDOCUMENT\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwalk_depth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracted \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m walks for \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m seed entities\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mlen\u001b[39m(walks_), \u001b[38;5;28mlen\u001b[39m(instances)))\n\u001b[0;32m    198\u001b[0m     walk_sentences \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m walks_]\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec_star\\lib\\RDF2Vec_Embed.py:201\u001b[0m, in \u001b[0;36mget_rdf2vec_walks\u001b[1;34m(ontology_file, mapping_file, walker_type, walk_depth, classes, config)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_rdf2vec_walks\u001b[39m(ontology_file, mapping_file, walker_type, walk_depth, classes, config):\n\u001b[1;32m--> 201\u001b[0m     kg, walker, classes \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_kg_walker\u001b[49m\u001b[43m(\u001b[49m\u001b[43montology_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43montology_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmapping_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mwalker_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwalker_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mwalk_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwalk_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;66;03m#kg.visualise()\u001b[39;00m\n\u001b[0;32m    208\u001b[0m     instances \u001b[38;5;241m=\u001b[39m [rdflib\u001b[38;5;241m.\u001b[39mURIRef(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m classes]\n",
      "File \u001b[1;32m~\\Documents\\OWL2Vec-Star\\owl2vec_star\\lib\\RDF2Vec_Embed.py:143\u001b[0m, in \u001b[0;36mconstruct_kg_walker\u001b[1;34m(ontology_file, mapping_file, walker_type, walk_depth, config)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mapping_file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtsv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmapping_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     lines \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './case_studies/Data/ALIN-cmt-conference.tsv'"
     ]
    }
   ],
   "source": [
    "import owl2vec4mappings\n",
    "\n",
    "# Parameters:\n",
    "# ontology_file1\n",
    "# ontology_file2\n",
    "# mapping_file\n",
    "# config_file\n",
    "# uri_doc\n",
    "# lit_doc\n",
    "# mix_doc\n",
    "# but there are some parameters you can change in config file\n",
    "gensim_model = owl2vec4mappings.extract_owl2vec_model(ontology_file1 = \"./case_studies/Data/cmt.owl\",\n",
    "                                                ontology_file2 = './case_studies/Data/Conference.owl',\n",
    "                                                mapping_file = './case_studies/Data/ALIN-cmt-conference.txt',\n",
    "                                                config_file = \"./default1.cfg\",\n",
    "                                                uri_doc = True, lit_doc = True, mix_doc = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"default1.cfg\")\n",
    "output_folder=config['DOCUMENT']['cache_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5472dcc3-c29a-4c6b-bf7e-c58b88291500",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim format\n",
    "#Run\n",
    "gensim_model.save(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73978c53-68cc-4b51-8419-e6908614459f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Txt format\n",
    "#Run\n",
    "gensim_model.wv.save_word2vec_format(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a78df2",
   "metadata": {},
   "source": [
    "## Loading embeddings and getting similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fdbcf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: loading Word2VecKeyedVectors object from ./caches_omim2ordo/ontology_3.embeddings\n",
      "INFO: loading wv recursively from ./caches_omim2ordo/ontology_3.embeddings.wv.* with mmap=r\n",
      "INFO: setting ignored attribute vectors_norm to None\n",
      "INFO: loading vocabulary recursively from ./caches_omim2ordo/ontology_3.embeddings.vocabulary.* with mmap=r\n",
      "INFO: loading trainables recursively from ./caches_omim2ordo/ontology_3.embeddings.trainables.* with mmap=r\n",
      "INFO: setting ignored attribute cum_table to None\n",
      "INFO: loaded ./caches_omim2ordo/ontology_3.embeddings\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'committee' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m KeyedVectors\u001b[38;5;241m.\u001b[39mload(output_folder\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124montology\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOCUMENT\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwalk_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, mmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m wv \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\n\u001b[1;32m----> 7\u001b[0m vector \u001b[38;5;241m=\u001b[39m \u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcommittee\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get numpy vector of a word\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVector for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommittee\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(vector)\n",
      "File \u001b[1;32mc:\\users\\sev_s\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:353\u001b[0m, in \u001b[0;36mBaseKeyedVectors.__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `entities`.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[0;32m    340\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(entities, string_types):\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(entity) \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m entities])\n",
      "File \u001b[1;32mc:\\users\\sev_s\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:471\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.get_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[1;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\sev_s\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\keyedvectors.py:468\u001b[0m, in \u001b[0;36mWordEmbeddingsKeyedVectors.word_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 468\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not in vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m word)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'committee' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Embedding vectors generated above\n",
    "model = KeyedVectors.load(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings\", mmap='r')\n",
    "wv = model.wv\n",
    "\n",
    "vector = wv['committee']  # Get numpy vector of a word\n",
    "print(\"Vector for 'committee'\")\n",
    "print(vector)\n",
    "print()\n",
    "\n",
    "#cosine similarity\n",
    "similarity = wv.similarity('committee', 'http://conference#Committee')\n",
    "print(\"Similarity between committee and http://conference#Committee\", similarity)\n",
    "print()\n",
    "\n",
    "similarity = wv.similarity('http://conference#Reviewer', 'reviewer')\n",
    "print(similarity)\n",
    "print()\n",
    "\n",
    "#Most similar cosine similarity\n",
    "result = wv.most_similar(positive=['reviewer', 'committee'])\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "#Most similar entities: cosmul\n",
    "result = wv.most_similar_cosmul(positive=['reviewer'])\n",
    "print(result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8526b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "ranking_results = []\n",
    "# save the scored candidate mappings in the same format as the original test.cands.tsv\n",
    "f = open(\"scored.test.cands.tsv\", \"w\")\n",
    "f.write(\"SrcEntity\" +'\\t'+ \"TgtEntity\" +'\\t'+ \"TgtCandidates\\n\")\n",
    "file = open(r\"C:\\Users\\sev_s\\Documents\\OWL2Vec-Star\\case_studies\\Data\\ncit2doid20223_test.cands.tsv\")\n",
    "\n",
    "for i in file.readlines()[1:]:\n",
    "    src_ref_class = i.split('\\t')[0]\n",
    "    tgt_ref_class = i.split('\\t')[1]\n",
    "    tgt_cands = i.split('\\t')[2]\n",
    "    # print(src_ref_class, tgt_ref_class, tgt_cands)\n",
    "    tgt_cands = eval(tgt_cands)  # transform string into list or sequence\n",
    "    scored_cands = []\n",
    "    for tgt_cand in tgt_cands:\n",
    "        # assign a score to each candidate with an OM system\n",
    "        try:\n",
    "            matching_score = model.wv.similarity(src_ref_class, tgt_cand)\n",
    "        except:\n",
    "            matching_score = 0\n",
    "        scored_cands.append([tgt_cand, matching_score])\n",
    "    ranking_results.append([src_ref_class, tgt_ref_class, scored_cands])\n",
    "    f.write(f'{src_ref_class}\\t{tgt_ref_class}\\t{scored_cands}\\n')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53ac11-4f6c-4bfa-be85-ede55e0f88db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "import numpy as np\n",
    "distance_results = []\n",
    "# save the scored candidate mappings in the same format as the original test.cands.tsv\n",
    "f = open(\"distance.test.cands.tsv\", \"w\")\n",
    "f.write(\"SrcEntity\" +'\\t'+ \"TgtEntity\" +'\\t'+ \"TgtCandidates\\n\")\n",
    "file = open(r\"C:\\Users\\sev_s\\Documents\\OWL2Vec-Star\\case_studies\\Data\\ncit2doid20223_test.cands.tsv\")\n",
    "\n",
    "for i in file.readlines()[1:]:\n",
    "    src_ref_class = i.split('\\t')[0]\n",
    "    tgt_ref_class = i.split('\\t')[1]\n",
    "    tgt_cands = i.split('\\t')[2]\n",
    "    # print(src_ref_class, tgt_ref_class, tgt_cands)\n",
    "    tgt_cands = eval(tgt_cands)  # transform string into list or sequence\n",
    "    scored_cands = []\n",
    "    for tgt_cand in tgt_cands:\n",
    "        # assign a score to each candidate with an OM system\n",
    "        try:\n",
    "            euc_dist = np.linalg.norm(model.wv[src_ref_class] - model.wv[tgt_cand])\n",
    "        except:\n",
    "            euc_dist = np.inf\n",
    "        scored_cands.append([tgt_cand, euc_dist])\n",
    "    distance_results.append([src_ref_class, tgt_ref_class, scored_cands])\n",
    "    f.write(f'{src_ref_class}\\t{tgt_ref_class}\\t{scored_cands}\\n')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f473ab-cb5a-436b-9d7c-3e7f68f9c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your Word2Vec model\n",
    "model = Word2Vec.load(output_folder+\"ontology\"+'_'+config['DOCUMENT']['walk_depth']+\".embeddings\")\n",
    "\n",
    "# Extract word vectors and corresponding words\n",
    "words = list(model.wv.vocab.keys())\n",
    "vectors = [model.wv[word] for word in words]\n",
    "\n",
    "# Apply PCA to reduce dimensionality\n",
    "num_dimensions = 2\n",
    "pca = PCA(n_components=num_dimensions)\n",
    "vectors_pca = pca.fit_transform(vectors)\n",
    "\n",
    "# Elbow method for finding number of clusters\n",
    "\n",
    "wcss = [] \n",
    "for i in range(1, 16): \n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 42)\n",
    "    kmeans.fit(vectors_pca) \n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,16), wcss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6644d3-2823-4989-84a9-dc2d6501e9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering\n",
    "num_clusters = 5  # You can change this based on your preference\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(vectors_pca)\n",
    "\n",
    "# Visualize the clusters\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "for i in range(num_clusters):\n",
    "    cluster_points = np.array([vectors_pca[j] for j in range(len(words)) if kmeans.labels_[j] == i])\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i], label=f'Cluster {i + 1}')\n",
    "\n",
    "plt.title('Train-Val+ AML union LOgmap')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e084a-a289-4a97-b358-c6111c0c772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Choose a random index within the range of the words list\n",
    "random_index = random.sample(range(len(words)), 30)\n",
    "\n",
    "vector = {\n",
    "    'X':vectors_pca[:,0],\n",
    "    'Y':vectors_pca[:,1],\n",
    "    'words': np.array(words),\n",
    "    'label':kmeans.labels_\n",
    "}\n",
    "\n",
    "sample_vectors_pca = np.array([vector['X'][random_index], vector['Y'][random_index]]).T\n",
    "sample_words = np.array(vector['words'][random_index])\n",
    "sample_cluster = np.array(vector['label'][random_index])\n",
    "\n",
    "# Visualize the clusters\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "markers = ['o', 'v', '^', '<', '>', 's', 'p', 'h', 'x', '+', 'd', '|', '_']\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_points = np.array([sample_vectors_pca[j] for j in range(len(sample_words)) if sample_cluster[j] == i])\n",
    "    plt.scatter(cluster_points[:,0], cluster_points[:,1], c=colors[i], marker=markers[i])\n",
    "\n",
    "for i, txt in enumerate(sample_words):\n",
    "    plt.annotate(txt, (sample_vectors_pca[i,0], sample_vectors_pca[i,1]), fontsize=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d4764-08ea-4c6a-bf45-5d38352e0659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466cd7e-8432-4fe7-8074-0ccbdc187154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
